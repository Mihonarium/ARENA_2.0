{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab # type: ignore\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "import os, sys\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install packages\n",
    "    %pip install einops\n",
    "    %pip install jaxtyping\n",
    "    %pip install transformer_lens\n",
    "    %pip install git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
    "\n",
    "    # Code to download the necessary files (e.g. solutions, test funcs)\n",
    "    import os, sys\n",
    "    if not os.path.exists(\"chapter1_transformers\"):\n",
    "        !wget https://github.com/callummcdougall/ARENA_2.0/archive/refs/heads/main.zip\n",
    "        !unzip /content/main.zip 'ARENA_2.0-main/chapter1_transformers/exercises/*'\n",
    "        sys.path.append(\"/content/ARENA_2.0-main/chapter1_transformers/exercises\")\n",
    "        os.remove(\"/content/main.zip\")\n",
    "        os.rename(\"ARENA_2.0-main/chapter1_transformers\", \"chapter1_transformers\")\n",
    "        os.rmdir(\"ARENA_2.0-main\")\n",
    "        os.chdir(\"chapter1_transformers/exercises\")\n",
    "else:\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "# Things that need to be done manually\n",
    "\n",
    "# Broken indentation at the very end of solutions file (put in if MAIN!)\n",
    "\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "import sys\n",
    "import torch as t\n",
    "from pathlib import Path\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformers\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"monthly_algorithmic_problems\" / \"july23_palindromes\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from monthly_algorithmic_problems.july23_palindromes.dataset import PalindromeDataset, display_seq\n",
    "from monthly_algorithmic_problems.july23_palindromes.model import create_model\n",
    "from plotly_utils import hist, bar, imshow\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `[1.B]` Monthly Algorithmic Challenge: Palindromes\n",
    "\n",
    "This marks the first of the (hopefully sequence of) monthly mechanistic interpretability challenges. I designed them in the spirit of [Stefan & Marius' challenges](https://www.lesswrong.com/posts/k43v47eQjaj6fY7LE/solving-the-mechanistic-interpretability-challenges-eis-vii-1), but with the more specific aim of working well in the context of the rest of the ARENA material, and helping people put into practice all the things they've learned so far.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "The following ARENA material should be considered essential:\n",
    "\n",
    "* **[1.1] Transformer from scratch** (sections 1-3)\n",
    "* **[1.2] Intro to Mech Interp** (sections 1-3)\n",
    "\n",
    "The following material isn't essential, but is very strongly recommended:\n",
    "\n",
    "* **[1.2] Intro to Mech Interp** (section 4)\n",
    "* **[1.4] Balanced Bracket Classifier** (all sections)\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Neel Nanda's post [200 COP in MI: Interpreting Algorithmic Problems](https://www.lesswrong.com/posts/ejtFsvyhRkMofKAFy/200-cop-in-mi-interpreting-algorithmic-problems) does a good job explaining the motivation behind solving algorithmic problems such as these. I'd strongly recommend reading the whole post, because it also gives some high-level advice for approaching such problems.\n",
    "\n",
    "The main purpose of these challenges isn't to break new ground in mech interp, rather they're designed to help you practice using & develop better understanding for standard MI tools (e.g. interpreting attention, direct logit attribution), and more generally working with libraries like TransformerLens.\n",
    "\n",
    "Also, they're hopefully pretty fun, because why shouldn't we have some fun while we're learning?\n",
    "\n",
    "## Logistics\n",
    "\n",
    "If this first problem is well-received, I'll try to post a new one every month. Because I think this one is on the easier side relatively speaking, I'll leave it open until the end of July (which at time of writing is 16 days). **My solution will be published on 31st July on this page**, at the same time as the next problem in the sequence. Future challenges will also be accompanied by a LessWrong post, but not this one (because it's experimental).\n",
    "\n",
    "If you try to interpret this model, you can send your attempt in any of the following formats:\n",
    "\n",
    "* Colab notebook\n",
    "* GitHub repo (e.g. with ipynb or markdown file explaining results)\n",
    "* Google Doc (with screenshots and explanations)\n",
    "* or any other sensible format.\n",
    "\n",
    "You can send your attempt to me (Callum McDougall) via any of the following methods:\n",
    "\n",
    "* The [Slack group](https://join.slack.com/t/arena-la82367/shared_invite/zt-1uvoagohe-JUv9xB7Vr143pdx1UBPrzQ), via a direct message to me\n",
    "* My personal email: `cal.s.mcdougall@gmail.com`\n",
    "* LessWrong message ([here](https://www.lesswrong.com/users/themcdouglas) is my user)\n",
    "\n",
    "**I'll feature the names of everyone who sends me a solution on this website, and also give a shout out to the best solutions.** It's possible that future challenges will also feature a monetary prize, but this is not guaranteed.\n",
    "\n",
    "\n",
    "## What counts as a solution?\n",
    "\n",
    "Going through the exercises **[1.4] Balanced Bracket Classifier** should give you a good idea of what I'm looking for. This model is much less complicated than the one in that exercise, so I'd have a higher standard for what counts as a full solution. In particular, I'd expect you to:\n",
    "\n",
    "* Describe a mechanism for how the model solves the task, in the form of the QK and OV circuits of various attention heads (and possibly any other mechanisms the model uses, e.g. the direct path, or nonlinear effects from layernorm),\n",
    "* Provide evidence for your mechanism, e.g. with tools like attention probabilities, targeted ablation / patching, or direct logit attribution.\n",
    "* (Optional) Include additional detail, e.g. identifying the linear subspaces that the model uses for certain forms of information transmission.\n",
    "\n",
    "## Model & Task\n",
    "\n",
    "The directory containing all the relevant files is `chapter1_transformers/exercises/monthly_algorithmic_problems/july23_palindromes`. At the bottom of this page is a block of setup code for you to run, in a similar style to the other exercises in this chapter.\n",
    "\n",
    "The notebook for training the model is `training_model.ipynb`. You can inspect this to see for yourself how the model was trained.\n",
    "\n",
    "Each sequence in the dataset looks like:\n",
    "\n",
    "```\n",
    "[start_token, a_1, a_2, ..., a_N, end_token]\n",
    "```\n",
    "\n",
    "where `start_token = 31`, `end_token = 32`, and each value `a_i` is a value in the range `[0, 30]` inclusive. \n",
    "\n",
    "Each sequence has a corresponding label, which is `1` if the sequence is a palindrome (i.e. `(a_1, a_2, ..., a_N) == (a_N, ..., a_2, a_1)`), and `0` otherwise.\n",
    "\n",
    "You can use the function `display_seq` to display a sequence in a more readable format (with any tokens that stop it from being a palindrome highlighted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">START |<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">04</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">16</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">06</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">27</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">00</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">10</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">00</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">06</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">22</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">01</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">26</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">27</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">29</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">16</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span>| END\n",
       "</pre>\n"
      ],
      "text/plain": [
       "START |\u001b[1;38;5;208m04\u001b[0m|\u001b[1;37m16\u001b[0m|\u001b[1;38;5;208m06\u001b[0m|\u001b[1;37m27\u001b[0m|\u001b[1;38;5;208m00\u001b[0m|\u001b[1;38;5;208m10\u001b[0m|\u001b[1;38;5;208m00\u001b[0m|\u001b[1;37m09\u001b[0m|\u001b[1;38;5;208m06\u001b[0m|\u001b[1;38;5;208m09\u001b[0m|\u001b[1;38;5;208m22\u001b[0m|\u001b[1;38;5;208m25\u001b[0m|\u001b[1;37m09\u001b[0m|\u001b[1;38;5;208m01\u001b[0m|\u001b[1;38;5;208m25\u001b[0m|\u001b[1;38;5;208m26\u001b[0m|\u001b[1;37m27\u001b[0m|\u001b[1;38;5;208m29\u001b[0m|\u001b[1;37m16\u001b[0m|\u001b[1;38;5;208m25\u001b[0m| END\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">START |<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">30</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">02</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">20</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">18</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">29</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">08</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">04</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">10</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">12</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">12</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">10</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">04</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">08</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">29</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">18</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">20</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">02</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">30</span>| END\n",
       "</pre>\n"
      ],
      "text/plain": [
       "START |\u001b[1;37m30\u001b[0m|\u001b[1;37m02\u001b[0m|\u001b[1;37m20\u001b[0m|\u001b[1;37m18\u001b[0m|\u001b[1;37m29\u001b[0m|\u001b[1;37m08\u001b[0m|\u001b[1;37m04\u001b[0m|\u001b[1;37m10\u001b[0m|\u001b[1;37m13\u001b[0m|\u001b[1;37m12\u001b[0m|\u001b[1;37m12\u001b[0m|\u001b[1;37m13\u001b[0m|\u001b[1;37m10\u001b[0m|\u001b[1;37m04\u001b[0m|\u001b[1;37m08\u001b[0m|\u001b[1;37m29\u001b[0m|\u001b[1;37m18\u001b[0m|\u001b[1;37m20\u001b[0m|\u001b[1;37m02\u001b[0m|\u001b[1;37m30\u001b[0m| END\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">START |<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">30</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">22</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">22</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">00</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">28</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">01</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">00</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">09</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">23</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">27</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">27</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">23</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">30</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">00</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">01</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">28</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">00</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">22</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">22</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">23</span>| END\n",
       "</pre>\n"
      ],
      "text/plain": [
       "START |\u001b[1;38;5;208m30\u001b[0m|\u001b[1;37m22\u001b[0m|\u001b[1;37m22\u001b[0m|\u001b[1;37m00\u001b[0m|\u001b[1;37m28\u001b[0m|\u001b[1;37m01\u001b[0m|\u001b[1;37m00\u001b[0m|\u001b[1;38;5;208m09\u001b[0m|\u001b[1;37m23\u001b[0m|\u001b[1;37m27\u001b[0m|\u001b[1;37m27\u001b[0m|\u001b[1;37m23\u001b[0m|\u001b[1;38;5;208m30\u001b[0m|\u001b[1;37m00\u001b[0m|\u001b[1;37m01\u001b[0m|\u001b[1;37m28\u001b[0m|\u001b[1;37m00\u001b[0m|\u001b[1;37m22\u001b[0m|\u001b[1;37m22\u001b[0m|\u001b[1;38;5;208m23\u001b[0m| END\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">START |<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">18</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">20</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">21</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">28</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">05</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">21</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">01</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">08</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">16</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">16</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">08</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">01</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">21</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">05</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">28</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">21</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">20</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">18</span>| END\n",
       "</pre>\n"
      ],
      "text/plain": [
       "START |\u001b[1;37m18\u001b[0m|\u001b[1;37m20\u001b[0m|\u001b[1;37m21\u001b[0m|\u001b[1;37m28\u001b[0m|\u001b[1;37m05\u001b[0m|\u001b[1;37m13\u001b[0m|\u001b[1;37m21\u001b[0m|\u001b[1;37m01\u001b[0m|\u001b[1;37m08\u001b[0m|\u001b[1;37m16\u001b[0m|\u001b[1;37m16\u001b[0m|\u001b[1;37m08\u001b[0m|\u001b[1;37m01\u001b[0m|\u001b[1;37m21\u001b[0m|\u001b[1;37m13\u001b[0m|\u001b[1;37m05\u001b[0m|\u001b[1;37m28\u001b[0m|\u001b[1;37m21\u001b[0m|\u001b[1;37m20\u001b[0m|\u001b[1;37m18\u001b[0m| END\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">START |<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">30</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">18</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">17</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">23</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">11</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">24</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">01</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">17</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">28</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">11</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">29</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">20</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">10</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>| END\n",
       "</pre>\n"
      ],
      "text/plain": [
       "START |\u001b[1;37m13\u001b[0m|\u001b[1;38;5;208m30\u001b[0m|\u001b[1;38;5;208m18\u001b[0m|\u001b[1;38;5;208m17\u001b[0m|\u001b[1;38;5;208m25\u001b[0m|\u001b[1;38;5;208m23\u001b[0m|\u001b[1;37m11\u001b[0m|\u001b[1;38;5;208m24\u001b[0m|\u001b[1;37m09\u001b[0m|\u001b[1;38;5;208m01\u001b[0m|\u001b[1;38;5;208m17\u001b[0m|\u001b[1;37m09\u001b[0m|\u001b[1;38;5;208m28\u001b[0m|\u001b[1;37m11\u001b[0m|\u001b[1;38;5;208m29\u001b[0m|\u001b[1;38;5;208m20\u001b[0m|\u001b[1;38;5;208m09\u001b[0m|\u001b[1;38;5;208m10\u001b[0m|\u001b[1;38;5;208m25\u001b[0m|\u001b[1;37m13\u001b[0m| END\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = PalindromeDataset(size=100, max_value=30, half_length=10)\n",
    "\n",
    "for i in range(5):\n",
    "    toks, is_palindrome = dataset[i]\n",
    "    display_seq(toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is is a 2-layer transformer with 2 attention heads, and causal attention. It was trained to predict the palindrome label at the `[END]` token for each sequence. You can load it in as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = section_dir / \"palindrome_classifier.pt\"\n",
    "\n",
    "model = create_model(\n",
    "    half_length=10, # this is half the length of the palindrome sequences\n",
    "    max_value=30, # values in palindrome sequence are between 0 and max_value inclusive\n",
    "    seed=42,\n",
    "    d_model=28,\n",
    "    d_head=14,\n",
    "    n_heads=2,\n",
    "    normalization_type=\"LN\",\n",
    "    d_mlp=None # this is an attn-only model\n",
    ")\n",
    "\n",
    "state_dict = t.load(filename)\n",
    "\n",
    "state_dict = model.center_writing_weights(t.load(filename))\n",
    "state_dict = model.center_unembed(state_dict)\n",
    "state_dict = model.fold_layer_norm(state_dict)\n",
    "state_dict = model.fold_value_biases(state_dict)\n",
    "model.load_state_dict(state_dict, strict=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to process the state dictionary is a bit messy, but it's necessary to make sure the model is easy to work with. For instance, if you inspect the model's parameters, you'll see that `model.ln_final.w` is a vector of 1s, and `model.ln_final.b` is a vector of 0s (because the weight and bias have been folded into the unembedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_final weight:  Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "\n",
      "ln_final, bias:  Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"ln_final weight: \", model.ln_final.w)\n",
    "print(\"\\nln_final, bias: \", model.ln_final.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Aside - the other weight processing parameters</summary>\n",
    "\n",
    "Here's some more code to verify that our weights processing worked, in other words:\n",
    "\n",
    "* The unembedding matrix has mean zero over both its input dimension (`d_model`) and output dimension (`d_vocab`)\n",
    "* All writing weights (i.e. `b_O`, `W_O`, and both embeddings) have mean zero over their output dimension (`d_model`)\n",
    "* The value biases `b_V` are zero (because these can just be folded into the output biases `b_O`)\n",
    "\n",
    "```python\n",
    "W_U_mean_over_input = einops.reduce(model.W_U, \"d_model d_vocab -> d_model\", \"mean\")\n",
    "t.testing.assert_close(W_U_mean_over_input, t.zeros_like(W_U_mean_over_input))\n",
    "\n",
    "W_U_mean_over_output = einops.reduce(model.W_U, \"d_model d_vocab -> d_vocab\", \"mean\")\n",
    "t.testing.assert_close(W_U_mean_over_output, t.zeros_like(W_U_mean_over_output))\n",
    "\n",
    "W_O_mean_over_output = einops.reduce(model.W_O, \"layer head d_head d_model -> layer head d_head\", \"mean\")\n",
    "t.testing.assert_close(W_O_mean_over_output, t.zeros_like(W_O_mean_over_output))\n",
    "\n",
    "b_O_mean_over_output = einops.reduce(model.b_O, \"layer d_model -> layer\", \"mean\")\n",
    "t.testing.assert_close(b_O_mean_over_output, t.zeros_like(b_O_mean_over_output))\n",
    "\n",
    "W_E_mean_over_output = einops.reduce(model.W_E, \"token d_model -> token\", \"mean\")\n",
    "t.testing.assert_close(W_E_mean_over_output, t.zeros_like(W_E_mean_over_output))\n",
    "\n",
    "W_pos_mean_over_output = einops.reduce(model.W_pos, \"position d_model -> position\", \"mean\")\n",
    "t.testing.assert_close(W_pos_mean_over_output, t.zeros_like(W_pos_mean_over_output))\n",
    "\n",
    "b_V = model.b_V\n",
    "t.testing.assert_close(b_V, t.zeros_like(b_V))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was trained to output the correct classification at the `END` token, in other words the value of the residual stream at `END` (post-layernorm) is mapped through `model.W_U` which has shape `(d_model, 2)`, and this gives us our classification logits for `(not palindrome, palindrome)`.\n",
    "\n",
    "A demonstration of the model working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">START |<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">04</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">16</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">06</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">27</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">00</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">10</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">00</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">06</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">22</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">01</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">26</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">27</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">29</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">16</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span>| END  -&gt;  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "START |\u001b[1;38;5;208m04\u001b[0m|\u001b[1;37m16\u001b[0m|\u001b[1;38;5;208m06\u001b[0m|\u001b[1;37m27\u001b[0m|\u001b[1;38;5;208m00\u001b[0m|\u001b[1;38;5;208m10\u001b[0m|\u001b[1;38;5;208m00\u001b[0m|\u001b[1;37m09\u001b[0m|\u001b[1;38;5;208m06\u001b[0m|\u001b[1;38;5;208m09\u001b[0m|\u001b[1;38;5;208m22\u001b[0m|\u001b[1;38;5;208m25\u001b[0m|\u001b[1;37m09\u001b[0m|\u001b[1;38;5;208m01\u001b[0m|\u001b[1;38;5;208m25\u001b[0m|\u001b[1;38;5;208m26\u001b[0m|\u001b[1;37m27\u001b[0m|\u001b[1;38;5;208m29\u001b[0m|\u001b[1;37m16\u001b[0m|\u001b[1;38;5;208m25\u001b[0m| END  ->  \u001b[1;36m0.000\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">START |<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">30</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">02</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">20</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">18</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">29</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">08</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">04</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">10</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">12</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">12</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">10</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">04</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">08</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">29</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">18</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">20</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">02</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">30</span>| END  -&gt;  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.995</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "START |\u001b[1;37m30\u001b[0m|\u001b[1;37m02\u001b[0m|\u001b[1;37m20\u001b[0m|\u001b[1;37m18\u001b[0m|\u001b[1;37m29\u001b[0m|\u001b[1;37m08\u001b[0m|\u001b[1;37m04\u001b[0m|\u001b[1;37m10\u001b[0m|\u001b[1;37m13\u001b[0m|\u001b[1;37m12\u001b[0m|\u001b[1;37m12\u001b[0m|\u001b[1;37m13\u001b[0m|\u001b[1;37m10\u001b[0m|\u001b[1;37m04\u001b[0m|\u001b[1;37m08\u001b[0m|\u001b[1;37m29\u001b[0m|\u001b[1;37m18\u001b[0m|\u001b[1;37m20\u001b[0m|\u001b[1;37m02\u001b[0m|\u001b[1;37m30\u001b[0m| END  ->  \u001b[1;36m0.995\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">START |<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">30</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">22</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">22</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">00</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">28</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">01</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">00</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">09</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">23</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">27</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">27</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">23</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">30</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">00</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">01</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">28</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">00</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">22</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">22</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">23</span>| END  -&gt;  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "START |\u001b[1;38;5;208m30\u001b[0m|\u001b[1;37m22\u001b[0m|\u001b[1;37m22\u001b[0m|\u001b[1;37m00\u001b[0m|\u001b[1;37m28\u001b[0m|\u001b[1;37m01\u001b[0m|\u001b[1;37m00\u001b[0m|\u001b[1;38;5;208m09\u001b[0m|\u001b[1;37m23\u001b[0m|\u001b[1;37m27\u001b[0m|\u001b[1;37m27\u001b[0m|\u001b[1;37m23\u001b[0m|\u001b[1;38;5;208m30\u001b[0m|\u001b[1;37m00\u001b[0m|\u001b[1;37m01\u001b[0m|\u001b[1;37m28\u001b[0m|\u001b[1;37m00\u001b[0m|\u001b[1;37m22\u001b[0m|\u001b[1;37m22\u001b[0m|\u001b[1;38;5;208m23\u001b[0m| END  ->  \u001b[1;36m0.000\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">START |<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">18</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">20</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">21</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">28</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">05</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">21</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">01</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">08</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">16</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">16</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">08</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">01</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">21</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">05</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">28</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">21</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">20</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">18</span>| END  -&gt;  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.997</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "START |\u001b[1;37m18\u001b[0m|\u001b[1;37m20\u001b[0m|\u001b[1;37m21\u001b[0m|\u001b[1;37m28\u001b[0m|\u001b[1;37m05\u001b[0m|\u001b[1;37m13\u001b[0m|\u001b[1;37m21\u001b[0m|\u001b[1;37m01\u001b[0m|\u001b[1;37m08\u001b[0m|\u001b[1;37m16\u001b[0m|\u001b[1;37m16\u001b[0m|\u001b[1;37m08\u001b[0m|\u001b[1;37m01\u001b[0m|\u001b[1;37m21\u001b[0m|\u001b[1;37m13\u001b[0m|\u001b[1;37m05\u001b[0m|\u001b[1;37m28\u001b[0m|\u001b[1;37m21\u001b[0m|\u001b[1;37m20\u001b[0m|\u001b[1;37m18\u001b[0m| END  ->  \u001b[1;36m0.997\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">START |<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">30</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">18</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">17</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">23</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">11</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">24</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">01</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">17</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">28</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">11</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">29</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">20</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">09</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">10</span>|<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold\">25</span>|<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; font-weight: bold\">13</span>| END  -&gt;  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "START |\u001b[1;37m13\u001b[0m|\u001b[1;38;5;208m30\u001b[0m|\u001b[1;38;5;208m18\u001b[0m|\u001b[1;38;5;208m17\u001b[0m|\u001b[1;38;5;208m25\u001b[0m|\u001b[1;38;5;208m23\u001b[0m|\u001b[1;37m11\u001b[0m|\u001b[1;38;5;208m24\u001b[0m|\u001b[1;37m09\u001b[0m|\u001b[1;38;5;208m01\u001b[0m|\u001b[1;38;5;208m17\u001b[0m|\u001b[1;37m09\u001b[0m|\u001b[1;38;5;208m28\u001b[0m|\u001b[1;37m11\u001b[0m|\u001b[1;38;5;208m29\u001b[0m|\u001b[1;38;5;208m20\u001b[0m|\u001b[1;38;5;208m09\u001b[0m|\u001b[1;38;5;208m10\u001b[0m|\u001b[1;38;5;208m25\u001b[0m|\u001b[1;37m13\u001b[0m| END  ->  \u001b[1;36m0.000\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    toks, is_palindrome = dataset[i]\n",
    "    \n",
    "    logits = model(toks.unsqueeze(0))[0, -1]\n",
    "    probs = logits.softmax(-1)\n",
    "    prob_palindrome = probs[1].item()\n",
    "\n",
    "    display_seq(toks, prob_palindrome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(dataset.toks)\n",
    "\n",
    "probs = logits[:, -1].softmax(-1)\n",
    "probs_palindrome = probs[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click on this dropdown for a hint on how to start (and some example code).</summary>\n",
    "\n",
    "The following code will display the attention patterns for each head, on a particular example.\n",
    "\n",
    "```python\n",
    "display_seq(dataset.toks[batch_idx], probs_palindrome[batch_idx])\n",
    "\n",
    "import circuitsvis as cv\n",
    "\n",
    "cv.attention.attention_patterns(\n",
    "    attention = t.concat([cache[\"pattern\", layer][batch_idx] for layer in range(model.cfg.n_layers)]),\n",
    "    tokens = dataset.str_toks[batch_idx],\n",
    "    attention_head_names = [f\"{layer}.{head}\" for layer in range(model.cfg.n_layers) for head in range(model.cfg.n_heads)],\n",
    ")\n",
    "```\n",
    "\n",
    "Find (1) a palindromic example, and (2) a non-palindromic example which is close to being palindromic (i.e. only 1 or 2 tokens are different). Then, compare the attention patterns for these two examples. Questions you might want to answer:\n",
    "\n",
    "* How do the attention patterns for numbers which are palindromic (i.e. they are the same as their mirror image) differ from the numbers which aren't?\n",
    "* How does information eventually get to the `[END]` token?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "```python\n",
    "import os; os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "import sys\n",
    "import torch as t\n",
    "from pathlib import Path\n",
    "\n",
    "# Make sure exercises are in the path\n",
    "chapter = r\"chapter1_transformers\"\n",
    "exercises_dir = Path(f\"{os.getcwd().split(chapter)[0]}/{chapter}/exercises\").resolve()\n",
    "section_dir = exercises_dir / \"monthly_algorithmic_problems\" / \"july23_palindromes\"\n",
    "if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))\n",
    "\n",
    "from monthly_algorithmic_problems.july23_palindromes.dataset import PalindromeDataset, display_seq\n",
    "from monthly_algorithmic_problems.july23_palindromes.model import create_model\n",
    "from plotly_utils import hist, bar, imshow\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_intro_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
